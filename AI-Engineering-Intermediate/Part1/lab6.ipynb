{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d7db0ff",
   "metadata": {},
   "source": [
    "# RAG with Vector Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71a12b8",
   "metadata": {},
   "source": [
    "## Just a quick definition\n",
    "## What are Vector Embeddings?\n",
    "\n",
    "Vector embeddings are numerical representations of text, images, or other data types that capture semantic meaning in a multi-dimensional space. When you input text into an AI system, it gets converted into a vector (a series of numbers) where similar concepts are positioned close to each other in this high-dimensional space.\n",
    "\n",
    "For example, the words \"dog\" and \"puppy\" would have vectors that are mathematically close to each other, while \"dog\" and \"airplane\" would be farther apart. This allows AI systems to understand relationships and similarities between different pieces of content, making them essential for tasks like search, recommendation systems, and retrieval-augmented generation (RAG).\n",
    "\n",
    "![Image](../../images/vector-embeddings.png)\n",
    "\n",
    "*Image courtesy of [Qdrant](https://medium.com/@qdrant/what-are-vector-embeddings-a7bda215702d)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e2c227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# langchain libriaries\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if OPENAI_API_KEY is None:\n",
    "    raise Exception(\"API key is missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4303a43e",
   "metadata": {},
   "source": [
    "## Step 1: Load your documents\n",
    "\n",
    "**Note:** There are other documents you can load in using LangChain. LangChain with its [DocumentLoaders](https://python.langchain.com/docs/concepts/document_loaders/) allows you to ingest data from a variety of sources like:\n",
    "- csv\n",
    "- xlsx\n",
    "- PDF\n",
    "- Webpages\n",
    "- Cloud Providers like AWS/Azure/GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b95463",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folders = glob.glob(\"docs/*\")\n",
    "\n",
    "# load in all the txt files in the directory\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(\n",
    "        path=folder,\n",
    "        glob=\"**/*.txt\",\n",
    "        loader_cls=TextLoader,\n",
    "        loader_kwargs={\"encoding\": \"utf-8\"}\n",
    "    )\n",
    "    folder_docs = loader.load()\n",
    "    for doc in folder_docs:\n",
    "        doc.metadata[\"doc_type\"] = doc_type\n",
    "    documents.extend(folder_docs)\n",
    "\n",
    "# load the documents\n",
    "# documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d105a77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceea844",
   "metadata": {},
   "source": [
    "## Step 2: Split documents into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6e1811",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd0f074",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[35:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20480b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    if \"Alex\" in chunk.page_content:\n",
    "        print(chunk.page_content)\n",
    "        print(chunk.metadata)\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4d6cbe",
   "metadata": {},
   "source": [
    "## Step 3: Generate vector embeddings for your documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0a30e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'gpt-4o-mini'\n",
    "embedding_model = 'text-embedding-3-large'\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2782715",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9a41b8",
   "metadata": {},
   "source": [
    "## Step 4: Create a vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e73028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clearing out any previous vectorstores to have a fresh start\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41024dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vector store created with {vectorstore._collection.count()} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4e87a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore._collection.get().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1810d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorstore._collection.get(include=['embeddings'])['embeddings'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8e95e1",
   "metadata": {},
   "source": [
    "## Step 4.5: Visualizing the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d051dc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prework\n",
    "\n",
    "collection = vectorstore._collection\n",
    "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "vectors = np.array(result['embeddings'])\n",
    "documents = result['documents']\n",
    "doc_types = [metadata['doc_type'] for metadata in result['metadatas']]\n",
    "colors = [['blue', 'green', 'red'][['about', 'employees', 'services'].index(t)] for t in doc_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7434f0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We humans find it easier to visalize things in 2D!\n",
    "# Reduce the dimensionality of the vectors to 2D using t-SNE\n",
    "# (t-distributed stochastic neighbor embedding)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 2D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='2D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x',yaxis_title='y'),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44db80a0",
   "metadata": {},
   "source": [
    "## Step 5: Building the RAG chatbot logic\n",
    "\n",
    "In its most basic form, RAG applications have 4 parts to them:\n",
    "\n",
    "1. The **LLM** engine\n",
    "2. The **memory** of the application\n",
    "3. The information **retriever**\n",
    "4. The **conversation chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7a149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# 1. The LLM engine\n",
    "llm = ChatOpenAI(model=MODEL, temperature=0.7)\n",
    "\n",
    "# 2. The memory of the application\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# 3. The information retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# 4. The conversation chain\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5013169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who won the Managing partner elite award in 2025?\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "answer = result['answer']\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95d22d1",
   "metadata": {},
   "source": [
    "Lets inspect the conversation chain a little deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c74148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import StdOutCallbackHandler\n",
    "\n",
    "\n",
    "# 1. The LLM engine\n",
    "llm = ChatOpenAI(model=MODEL, temperature=0.7)\n",
    "\n",
    "# 2. The memory of the application\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# 3. The information retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# 4. The conversation chain\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    callbacks=[StdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc77d8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who won the Managing partner elite award in 2025?\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "answer = result['answer']\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f139ce36",
   "metadata": {},
   "source": [
    "## Step 6: Giving our AI a User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d556b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a11f625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(question, history=[]):\n",
    "    \"\"\"\n",
    "    Function to handle the chat interaction.\n",
    "    \"\"\"\n",
    "    # Invoke the conversation chain with the updated history\n",
    "    result = conversation_chain.invoke({\"question\": question, \"chat_history\": history})\n",
    "    \n",
    "    # Extract the answer from the result\n",
    "    answer = result['answer'] \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28754c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadd6e6b",
   "metadata": {},
   "source": [
    "# Your Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfb38f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
