{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e9e9106",
   "metadata": {},
   "source": [
    "# Prompt Engineering\n",
    "\n",
    "## Just a quick definition\n",
    "\n",
    "Prompt engineering is the process of designing and refining input prompts to guide large language models (LLMs) toward producing accurate, relevant, and useful outputs. It involves crafting clear instructions, providing context, and sometimes including examples to achieve the desired response from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b60c1b",
   "metadata": {},
   "source": [
    "### Prompt Elements\n",
    "\n",
    "A prompt contains any of the following elements:\n",
    "\n",
    "**Instruction** - a specific task or instruction you want the model to perform\n",
    "\n",
    "**Context** - external information or additional context that can steer the model to better responses\n",
    "\n",
    "**Input Data** - the input or question that we are interested to find a response for\n",
    "\n",
    "**Output Indicator** - the type or format of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5240aeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from openai import OpenAI\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b71096",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "# Call OpenAI model response function\n",
    "\n",
    "def get_response(prompt: str, model: str, response_id: Optional[int] = None) -> tuple[str, int]:\n",
    "    response = client.responses.create(\n",
    "        model=model,\n",
    "        previous_response_id=response_id,\n",
    "        input=[{\"role\": \"user\",\"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    return response.output_text, response.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d817ff0",
   "metadata": {},
   "source": [
    "# Prompting Methods\n",
    "\n",
    "### 1. Zero-shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552f847d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede57ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\n",
    "\n",
    "Classify the following text into one of the following categories:\n",
    "1. Technology\n",
    "2. Science\n",
    "3. Art\n",
    "4. History\n",
    "\n",
    "Text: The advancements in quantum computing have the potential to revolutionize the field of cryptography, enabling faster processing and more secure communication methods.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Call the response function\n",
    "response_text, response_id = get_response(PROMPT, MODEL)\n",
    "display(Markdown(response_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a2efc7",
   "metadata": {},
   "source": [
    "### 2. Few-shot/Multi-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5991b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\n",
    "\n",
    "Classify the following text into one of the following categories:\n",
    "1. Technology\n",
    "2. Science\n",
    "3. Art\n",
    "4. History\n",
    "\n",
    "Text: The advancements in quantum computing have the potential to revolutionize the field of cryptography, enabling faster processing and more secure communication methods.\n",
    "\n",
    "EXAMPLES\n",
    "\n",
    "Q: The invention of the printing press in the 15th century allowed for the mass production of books and the spread of knowledge.\n",
    "Technology\n",
    "\n",
    "Q: The discovery of penicillin in 1928 marked the beginning of modern antibiotics and has saved countless lives.\n",
    "Science\n",
    "\n",
    "Q: The Mona Lisa, painted by Leonardo da Vinci in the 16th century, is one of the most famous works of art in history.\n",
    "Art\n",
    "\n",
    "Q: The fall of the Berlin Wall in 1989 was a significant event in world history, symbolizing the end of the Cold War.\n",
    "History\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Call the response function\n",
    "response_text, response_id = get_response(PROMPT, MODEL)\n",
    "display(Markdown(response_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd0ccb2",
   "metadata": {},
   "source": [
    "### 3. Chain-of-Thought (CoT) Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764f385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad example\n",
    "MODEL = \"gpt-3.5-turbo\"\n",
    "PROMPT = \"\"\"\n",
    "\n",
    "A train leaves City A heading towards City B at 60 miles per hour.\n",
    "At the same time, another train leaves City B heading towards City A at 40 miles per hour.\n",
    "The cities are 300 miles apart. However, after 1 hour, the train from City A increases its \n",
    "speed by 20 miles per hour, and the train from City B decreases its speed by 10 miles per hour. \n",
    "How long after departure will the two trains meet?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Call the response function\n",
    "response_text, response_id = get_response(PROMPT, MODEL)\n",
    "display(Markdown(response_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd811932",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-3.5-turbo\"\n",
    "PROMPT = \"\"\"\n",
    "\n",
    "A train leaves City A heading towards City B at 60 miles per hour.\n",
    "At the same time, another train leaves City B heading towards City A at 40 miles per hour.\n",
    "The cities are 300 miles apart. However, after 1 hour, the train from City A increases its \n",
    "speed by 20 miles per hour, and the train from City B decreases its speed by 10 miles per hour. \n",
    "How long after departure will the two trains meet?\n",
    "\n",
    "Let's solve this problem step by step:\n",
    "1. Identify the speeds of both trains before and after the first hour.\n",
    "2. Calculate the distance covered by both trains in the first hour.\n",
    "3. Determine the remaining distance after the first hour.\n",
    "4. Calculate the new speeds of both trains after the first hour.\n",
    "5. Find out how much time it takes for the trains to meet after the first hour.\n",
    "6. Add up the total time taken for the trains to meet.\n",
    "\n",
    "Show all calculations and reasoning clearly.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Call the response function\n",
    "response_text, response_id = get_response(PROMPT, MODEL)\n",
    "display(Markdown(response_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19aa5d7",
   "metadata": {},
   "source": [
    "# Tips for better prompts\n",
    "\n",
    "### Tip 1: Write clear and concise instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b05bf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"I need to buy a birthday present for my sister\"\n",
    "\n",
    "# Call the response function\n",
    "response_text, response_id = get_response(PROMPT, MODEL)\n",
    "display(Markdown(response_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0a0efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"I need to buy a birthday present for my sister. She is 12 years old and is interested in ponies. Reply only with 2 options.\"\n",
    "\n",
    "# Call the response function\n",
    "response_text, response_id = get_response(PROMPT, MODEL)\n",
    "display(Markdown(response_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9434041a",
   "metadata": {},
   "source": [
    "### Tip 2: Split your task into simpler subtasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17bf448",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\n",
    "\n",
    "Write an article about the importance of muscle for skeletal health and mobility in old age.\n",
    "\n",
    "Follow these guidelines:\n",
    "- Jot down high-level bullets on the subject\n",
    "- Write a short introduction\n",
    "- Write a detailed section on the importance of muscle for skeletal health\n",
    "- Write a detailed section on the importance of muscle for mobility\n",
    "- Write a conclusion summarizing the key points\n",
    "- Use markdown formatting\n",
    "- Use a friendly and engaging tone\n",
    "- Use simple language that is easy to understand\n",
    "- Use a maximum of 500 words\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f0d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the response function\n",
    "response_text, response_id = get_response(PROMPT, MODEL)\n",
    "display(Markdown(response_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93eb81f",
   "metadata": {},
   "source": [
    "### Tip 3: Markdown Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4a9c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c2dfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\n",
    "\n",
    "You will be tasked to build a simple application using Python and Flask.\n",
    "\n",
    "You MUST plan effectively, thinking through each step before you write code.\n",
    "\n",
    "I want you to build an app that, upon clicking a button, will generate random Yoda quotes.\n",
    "\n",
    "# Workflow\n",
    "\n",
    "## High-level problem solving strategy\n",
    "1. Deeply understand the user's requirements.\n",
    "2. Break down the requirements into smaller tasks.\n",
    "3. For each task, think through the implementation details.\n",
    "4. Write code for each task, ensuring it works correctly.\n",
    "5. Test the code to ensure it meets the requirements.\n",
    "\n",
    "## Code implementation strategy\n",
    "1. Start with the main application structure.\n",
    "2. Implement the core functionality.\n",
    "3. Add any additional features as needed.\n",
    "\n",
    "## Other notes:\n",
    "1. The app should look sleek and modern\n",
    "2. It should feel snappy\n",
    "3. Do not ask any questions\n",
    "4. Give me the entire code in one file\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d8428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the response function\n",
    "response_text, response_id = get_response(PROMPT, MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27d575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(response_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5ca2a5",
   "metadata": {},
   "source": [
    "Let's compare with bad prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35135d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26719344",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"I want you to build an app that, upon clicking a button, will generate random Yoda quotes. The app should be sleek and modern and snappy. Give me all the code in one python file.\"\n",
    "\n",
    "# Call the response function\n",
    "response_text, response_id = get_response(PROMPT, MODEL)\n",
    "display(Markdown(response_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a15a39",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- [GPT-4.1 Prompting Guide](https://arxiv.org/pdf/2406.13121)\n",
    "- [Lee et al](https://arxiv.org/pdf/2406.13121) - More complex prompt structures\n",
    "- [Best Practices for Prompt Engineering by OpenAI API](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api)\n",
    "- [Prompt Engineering Repo](https://github.com/dair-ai/Prompt-Engineering-Guide) - A fantastic resource on more advanced prompt engineering tactics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fad314b",
   "metadata": {},
   "source": [
    "## Your Turn: Build an App with LLMs and Good Prompt Engineering\n",
    "\n",
    "Now that you've learned about prompt engineering and seen examples of how prompt quality affects LLM outputs, it's time to put your skills into practice!\n",
    "\n",
    "### Instructions\n",
    "- Think of a simple app or tool you want to build using an LLM (for example: a quiz generator, a text summarizer, a code explainer, a recipe recommender, etc.).\n",
    "- Write a prompt for the LLM that follows the good prompt engineering principles discussed:\n",
    "  - Be clear and specific about the task.\n",
    "  - Provide context and examples if needed.\n",
    "  - Specify the desired output format.\n",
    "  - Break down complex tasks into steps.\n",
    "- Use the provided `get_response` function to interact with the LLM and build your app logic in Python.\n",
    "- Display the results in a user-friendly way (e.g., using Markdown output).\n",
    "\n",
    "---\n",
    "\n",
    "Try to:\n",
    "- Document your approach and prompt design.\n",
    "- Experiment with different prompt styles and see how the output changes.\n",
    "- Share your app and prompt with others for feedback!\n",
    "\n",
    "Add your code to the `AI-Engineering-Intermediate/Part1/community-contributions` folder and open up a PR. All the instructions for uploading your contributions can be found in the [CONTRIBUTING](../../CONTRIBUTING.md) file.\n",
    "\n",
    "**BONUS:** Post about the app that you have built using your chosen LLM and tag the [SuperDataScience Community LinkedIn page](https://www.linkedin.com/showcase/superdatascience-community-projects/) for a chance to have your post shared with the official SDS page (both the community projects page and main [SDS page](https://www.linkedin.com/showcase/superdatascience)) and have your post seen by **everyone** who follows SDS on LinkedIn (~100,000 followers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29d290d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
